# [9/26] 3주차 - 조회 수 시스템 설계하기

생성일: 2025년 9월 10일 오후 1:02

## 기능적 요구사항

### 1. 조회수 카운팅

- 사용자가 콘텐츠를 조회할 때마다 **실시간으로 조회수 증가**
- **중복 조회 방지**: 같은 사용자가 **짧은 시간 내 중복 조회 시 1회만 카운트**  ⇒ “짧은 시간” 어떻게 ?
- **Bot 필터링**: 검색 엔진 크롤러, 스크래핑 봇의 조회는 제외
- **유효 조회 판정**: 최소 체류시간(3초) 이상인 경우만 카운트 ⇒ 유저 별 어떻게 시간 측정 ?

### 2. 조회수 조회 및 표시

- **실시간 조회수** 표시 (사용자가 새로고침하면 최신 조회수 확인 가능)
- **응답 속도**: 조회수 표시 **5ms 이내**

### 3. 통계 및 분석 기능

- **시간별 통계**: 시간/일/주/월별 조회수 집계  ⇒ 조회수 저장해야
- **인기 콘텐츠 랭킹**: 실시간/일간/주간 인기 순위
- **지역별 분석**: 국가/도시별 조회 패턴 (Optional)

### 규모 및 성능 요구사항

| 지표 | 요구사항 |
| --- | --- |
| **일 총 조회 요청** | **10억 건** |
| **피크 시간 TPS** | **초당 100만 조회** (오후 8-10시) |
| **동시 활성 사용자** | **100만 명** |
| **총 콘텐츠 수** | **1000만 개** |
| **조회수 증가 처리 시간** | **10ms 이내** |
| **조회수 조회 응답 시간** | **5ms 이내** |
| **실시간 반영 지연** | **최대 30초** |
- 대략적인 데이터 비율정도만 맞춰서 생각하시길 (일 총 조회 요청당 활성 사용자 비율은 10:1 이구나 대략 이런 느낌)

## 비기능적 요구사항

### **정확성 vs 성능의 균형**

- **실시간 정확도**: **95%** (약간의 오차 허용)
- **최종 정확도**: **99.9%** (배치 보정 후)
- **중복 제거율**: **90%** (완벽한 중복 제거보다는 성능 우선)

### **확장성 및 가용성**

- **시스템 가용률**: **99.9%** 이상
- **수평 확장**: 트래픽 증가 시 서버 추가로 선형 확장 가능
- **장애 복구**: 조회수 데이터 유실 없이 5분 이내 복구

### **데이터 보호**

- **조회수 조작 방지**: 비정상적인 조회 패턴 탐지 및 차단
- **데이터 백업**: 시간별 증분 백업, 일별 전체 백업

### 고민해야 할점

- 실시간성 vs 정확성
- 중복 조회 판정
- 대용량 트래픽 처리
- 데이터 일관성

## 단계 별로 생각 해야 할 포인트

### 초당 1000건일때

- 단순하게 접근
- 단일 DB로 처리

### 초당 10000건일 때

- 캐싱을 한다면?

### 초당 50,000건 이상 일 때

- 글로벌 서비스이다면?

---

<aside>

### 요구사항 요약

- **규모**: 일 10억 뷰, 피크 100만 RPS, 동시활성 100만, 콘텐츠 수 1천만
- **지연**: 증가 처리 10ms 이내(수집 ACK), 조회 표시 5ms 이내(서버측), 실시간 반영 ≤ 30s
- **정확도**: 실시간 95%, 최종 99.9%(배치 보정), 중복 제거율 90%+
    - 중복 판단: **60초**(콘텐츠×사용자 단위)
    - 유효 조회 판단 : **체류 ≥ 3s,** 확정 이벤트 도착 시 카운트
</aside>

### 전체 아키텍처

### **1. 조회수 수집**

- Client SDK (웹 브라우저/앱)
: 사용자가 컨텐츠를 클릭하거나 들어가면, SDK가 `view_start` 이벤트(컨텐츠 및 유저 식별자, IP, 시간 등 포함)를 서버로 전송
- Edge Collector / API Gateway
    - 크롤러, 너무 빠른 반복 요청, 이상한 IP 대역 등이 이 단계에서 필터링됨
    - 정상 이벤트만 내부 메시지 큐(Kafka)로 전달
    - 응답은 매우 빠르게 ACK만 주고 끝낸다. 여기서 조회수 증가 여부는 확정하지 않고, 나중에 처리
- Kafka
    - 메시지 큐를 통해 조회수 데이터 보장
    - 메시지는 `content_id` 기준으로 파티션을 나눠 고르게 분산 ⇒ 이후 모든 처리는 Kafka에서 Read 하는 방식으로 진행됨
    

### **2. 조회수 처리**

- **Stream Proccessor (Kafka Streams)**
    
    <aside>
    
    ### **Stream Proccessor**
    
    일반적으로 우리가 다루는 데이터는 정적인 DB 테이블 혹은, 로그 파일 같은 배치 데이터들이다.  
    하지만 조회수처럼 계속해서 들어오는 이벤트는 파일 단위로 쌓일 때까지 기다리면 늦다. 
    
    따라서 ‘들어오는 순간 바로 처리’하는 시스템이 필요한데, 바로 `Stream Proccessor` 가 그 역할을 한다. 
    
    - **Apache Flink**
        - 오픈소스 실시간 처리 엔진
        - 초당 수백만 건 이상 처리 가능, 상태 관리(stateful) 잘됨, 정확성 보장(Exactly-Once) 모드도 지원
        - 많이 쓰는 이유: Kafka랑 잘 붙고, 복잡한 집계/윈도우 처리에 강함
    - **Kafka Streams**
        - Kafka 토픽 안의 메시지를 실시간으로 변환/집계하는 데 특화됨
        - Flink보다는 단순하지만, 운영은 간단(별도 클러스터 필요 없음, 애플리케이션 코드에 라이브러리처럼 붙임)
    </aside>
    
    - **최소 3초 체류 확인** : `view_start` 이벤트만 들어오면 보류 상태로 두고, `view_confirm` 이벤트가 3초 후 들어오면 **실제 조회로 카운트**
    - **중복 판단 → 제거** : 같은 유저가 60초 안에 같은 콘텐츠를 여러 번 보면, 한 번만 카운트
        
        ```smalltalk
        [동작 방식] 
        1. 유저가 컨텐츠 조회 => Stream Processor가 '유효한 조회'인지 확인
        2. Reids or Kafka Stream 에 중복 키 조회 => 키 이름 : duplicate:{content_id}:{user_key} 
        2-1. 키가 없다면 새로운 중복 키 생성 => SETNX duplicate:article123:userA 1 EX 60
        
        Q] 왜 Redis인가? 
        - 속도 : 초당 100만 건 같은 피크 트래픽에서도 SETNX + EXPIRE 연산은 매우 빠르다. 
        - 자동 만료 : TTL 덕분에 중복 판단 기준인 60초가 지나면 새로운 조회로 카운트하기에, 개발자가 신경쓰지 않아도 된다.
        - 확장성 : Redis Cluster로 여러 샤드에 나누면 메모리/처리량 확장이 쉽다. 
        
        Q] 다른 대안
        : InfluxDB 같은 시계열 DB에 (content_id, user_key) 이벤트 기록 & TTL => Redis 보다 느리고, 확장도 힘듦(초당 100만 RPS 단독 처리 어려움)
        
        Q] Redis의 한계
        : 피크에 동시 6000만(초당 100만 RPS X 중복 판정 시간) (content,user)조합이 생기고, 키 하나라가 100B라고 가정하면, 최소 6GB 메모리가 필요하다. 
          Cluster라면 분산 처리가 가능하지만, 여전히 부담히 될 수 있는 수치이다. 
          
          따라서, 실제 서비스에서는 상위 노출 컨텐츠에 대해선 높은 정확도가 중요하므로, 위의 중복 키(SETNX) 방식을 택하고, 
          하위(대부분 콘텐츠)는 조회가 분산되므로 약간의 오차를 허용하는 'Bloom Filter(근삿값)' 방식을 택한다고 한다.
        ```
        
    

### **3. 조회수 저장 ⇒ 실시간으로 화면에 보여줄 ‘현재 조회수’ 저장**

**[단기간 집계용]**

- **Redis Cluster** : `INCR`(+1 연산) , `GET` 연산이 빠름. But 장기 보관 부적합 및 메모리 비용 높음
- Memcached : 단순 key-value 캐시, 저비용, 수평 확장 용이. But 영속성 없음

 ⇒ 단기간(최근) + 장기간 용도 분리

**[장기간 집계용]** 

- **ClickHouse** / Druid
    - 장점: 대량 이벤트 집계에 최적, 같은 열에 유사 데이터 반복되기에 압축하여 매우 효율적인 메모리 사용, 초당 수십 GB 데이터 처리 가능
            컬럼 많은 경우에 최적화, RDB 속도의 최소 100배라고 함.함
    - 단점: 운영/튜닝 필요, 쿼리 모델 단순해야 효율적, 조회 연산 제외하고 지원 제한적
    
    [[ClickHouse] 클릭하우스 란?](https://vprog1215.tistory.com/391)
    

> 따라서, Redis Cluster 에서 최근 조회수 집계 및 ClickHouse 에서 장기 집계하는 방식
> 

### 4. 조회수 보정

**[요구사항]** 

> **실시간 정확도 ⇒ `보정` 후의 최종 정확도** 
실시간 카운팅은 속도 때문에 95% 정확도만 보장 (중복 완벽 제거 불가, confirm 누락 가능 등으로 약간의 오차 허용)
> 

**[주기 및 전략]** 

- **일 단위 (자정~새벽)**: 최종 통계용, UI도 보정 후 갱신
- **시간 단위 (1시간마다)**: 인기 콘텐츠용, 근사치 보정 ⇒ 실시간 스트림에서 추가 검증(SETNX 모드)으로 보정 부담 줄임

**[어떻게?]**

- 보정 과정에서는 중복 판정/체류시간 조건을 SQL 레벨에서 **정확하게** 다시 수행하여 중복 제거 및 Confirm 누락 반영

**[정리]**

**실시간(95%)** : Redis에서 빠르게 보여줌.
**최종(99.9%) :**  Kafka/ClickHouse 원본 이벤트를 기준으로 하루 1회 이상 Batch 방식으로 재집계 및 갱신.

### 5. 조회수 조회

**[기본 흐름]**

- 사용자가 콘텐츠 페이지를 새로고침하거나 진입
- **View Count API** 호출 → 서버가 조회수 반환
- 서버는 Redis에서 조회수 조회 후 응답

**[캐싱 전략]** 

- 보통 컨텐츠의 경우 ⇒ Redis에서 바로 읽음(5ms 이내 응답 보장)
- **인기 컨텐츠의 경우**
    - **Redis 부하 방지를 위한 CDN 캐시 도입 ⇒ 1~5초 TTL 캐시로 묶어서 조회 수 갱신, 수백만 명이 동시에 봐도 캐시에서 바로 응답**
        
        <aside>
        
        **[가정]**
        
        > 피크 트래픽 **100만 rps** (1초에 100만 명이 동시에 조회), 인기 콘텐츠 1개에 집중된다고 가정, CDN 캐시 TTL = **3초**
        > 
        
        **캐시가 없다면,,**
        모든 요청이 API → Redis로 들어가게 되고, 초당 100만 GET을 처리해야 한다.
        
        **CDN 과 같은 컨텐츠 전용 캐시가 있다면**
        
        - **최초 요청 시 API가 Redis에서 조회 → 응답 + 캐시에 저장**
        이후 **3초 동안의 모든 요청**은 CDN 캐시에서 바로 응답
        - 100만 rps × 3초 = **300만 요청이 Redis로 가지 않고 CDN에서 처리됨.**
        
        즉, Redis 입장에선 3초에 한 번만 실제 조회 처리 → 실질 부하가 **100만 rps → 1/3초당 1회** 수준으로 줄어듦.
        (물론 여러 인기 콘텐츠가 동시에 있으면, 각각 TTL 주기로 부하가 분산됨)
        
        </aside>
        
    - 조회수는 약간 늦게 반영되지만 사용자는 거의 차이를 못 느낌
    - 만약, 조회수 보정으로 숫자가 줄어들어야한다면 ⇒ 조회수가 줄어드는건  UX상 너무 안좋음. 따라서 일단 캐시에 저장된 값을 보여주고
    내부적으로는 다시 맞춰서 나중에 크게 뛰는 식으로 처리 한다고 함.

### 총 컨텐츠 수 1000만개는 어느 정도일까?

당연히, Redis에 **모든 콘텐츠를 항상 들고 있으면 메모리 폭탄일 것**
⇒ 컨텐츠 카운터 하나당 16바이트(키+값+오버헤드)만 잡아도, 1,000만 × 16B ≈ 160MB
   실제로는 key 오버헤드, 클러스터 샤딩 메타, 복제본 포함하면 수 GB 이상이라고 함.

⇒ 하지만,  **실제로 많이 조회되는 건 상위 1~5% 콘텐츠**에 집중됨

- **Hot 콘텐츠만 Redis 유지
:** “최근 1주일간 조회 발생” 같은 기준으로만 Redis에 올려두고, 장기 미사용 콘텐츠는 디스크 기반 DB/ClikeHouse 에서 조회 시 Lazy Load.
- **Base + Delta 구조**:
    - Base: 장기 스토리지(ClickHouse)에 저장 ⇒ 컬럼 기반이라 10억 건/day X 30일치 데이터라도 TB 단위도 충분히 관리 가능
    - Delta: Redis에만 유지하다 주기적으로 합산 ⇒ 덕분에 비활성 콘텐츠는 Redis 메모리 차지 안 함

- **조회의 경우에도**
    - 사용자가 특정 콘텐츠를 조회할 때만 Redis에 키를 만들고, 없으면 “0 or base_count” 반환 후 등록
    - **ZSET(랭킹)** 같은 자료구조를 쓸 때는 1,000만 전부 다 들고 있을 필요 없음 → 상위 N만 유지

## 단계별로 생각해야할 포인트

**1. 초당 1K RPS (단순 서비스 규모)**

- **단일 DB 샤드로 처리 가능**: RDB(MySQL/Postgres)에서도 충분히 감당 가능
- 다만 **중복 제거 로직**을 DB에서 직접 처리하면 락/부하 이슈가 있으니, **애플리케이션 레벨 캐시** (+ Redis?)
- 보정 로직은 Batch 쿼리(하루 한 번)로도 충분

**2. 초당 10k RPS (중간 규모, 캐싱 필수)**

- Redis 캐시 계층 추가 필요(Redis + CDN 캐시(TTL 1~3초) 적극 고려해야)
- 중복 제거: Redis `SETNX`/`EXPIRE` 기반으로 충분히 감당 가능

**3. 초당 50K RPS (글로벌/대규모 서비스)**

- 반드시 **메시지 큐(Kafka)**, **스트림 처리기(Kafka Streams)**, **다중 샤드 Redis Cluster** 필요
- **글로벌 서비스**라면 리전별 수집기 배치 + Kafka MirrorMaker(리전 간 이벤트 싱크) 고려해야
- **지연**: 사용자와 가까운 POP(Edge Collector)에서 이벤트 수집 후 **중앙 Kafka**로 전송 → 전 세계에서 100ms 이내 ACK 보장
- **데이터 정합성**: 각 리전은 지역 Redis에 실시간 조회수 저장, 글로벌 집계는ClickHouse 에서 리전별 집계 합산