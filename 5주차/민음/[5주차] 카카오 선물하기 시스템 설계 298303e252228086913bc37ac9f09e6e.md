# [5주차] 카카오 선물하기 시스템 설계

생성일: 2025년 10월 26일 오후 3:30

## **기능적 요구사항**

## **📊 규모 및 성능 요구사항**

**1. 선물 예약 기능**

- 사용자는 **특정 날짜/시간**에 **선물이 발송되도록 예약**할 수 있다
- 예약 가능 기간: 최소 10분 후 ~ 최대 1년 후
- 발송 시간 지정: **날짜 + 시간 또는 날짜만 (기본 00:00)**
- 개인 메시지 첨부 가능 (최대 500자)
- 수신자 정보: 전화번호 또는 카카오톡 ID

**2. 선물 발송 처리**

- 예약 시간에 **정확히 (±1분 이내)** 발송
- 발송 방식: 카카오톡 메시지, SMS, 이메일
- **기프티콘 번호 자동 생성** 및 전달
- 발송 실패 시 **자동 재시도** (최대 3회)

**3. 선물 수령 및 사용**

- 수신자는 메시지 내 링크로 선물 확인
- **30일 내 사용** 기한 (상품별 상이)
- 사용 전 **타인 양도 가능** (1회 한정)
- 미사용 시 **자동 환불** 처리

**4. 예약 관리 기능**

- 예약 내역 **조회 및 상세 확인**
- 발송 **전까지 취소 가능** (취소 수수료 없음)
- 발송 **1시간 전까지 수정 가능** (수신자, 메시지, 시간)
- 예약 상태 실시간 추적

**5. 알림 기능**

- 발송자: 예약 확인, 발송 완료, 수령 확인
- 수신자: 선물 도착, 만료 임박 (D-3, D-1)
- 알림 채널: 앱 푸시, 카카오톡, SMS

| **일 예약 건수** | **5만건** | **50만건** |
| --- | --- | --- |
| 동시 접속자 | 5천명 | 5만명 |
| 시간당 발송 | 2천건 | 2만건 |

**피크 타임 (실생활 패턴)**

`🎄 12월 (크리스마스): 평소 대비 10배 ↑ 💝 2월 14일 (밸런타인데이): 당일 00시 집중 (50배 ↑) 🎂 생일 시즌: 매일 오전 9시, 자정 집중 🧧 설날/추석: 명절 당일 오전 집중`

**특정 시간대 집중도**

- **자정 (00:00)**: 전체 트래픽의 **30%** (생일 발송)
- **오전 9시**: 전체 트래픽의 **15%** (아침 인사)
- **저녁 8시**: 전체 트래픽의 **10%** (퇴근 시간)

**성능 요구사항**

- 예약 등록 응답 시간: **500ms 이내**
- 발송 정확도: 예약 시간 **±1분 이내**
- 발송 성공률: **99% 이상**
- 알림 지연 시간: **30초 이내**

## **비기능적 요구사항 (Non-Functional Requirements)**

## **핵심 설계 고민 포인트**

**정확성 (Accuracy)**

- **발송 시간 준수**: 예약 시간 오차 ±1분 이내
- **중복 발송 방지**: 동일 선물 1회만 발송 (멱등성)
- **결제 정합성**: 예약-발송-환불 금액 일치
- **데이터 일관성**: 예약 정보와 발송 이력 동기화

**확장성 (Scalability)**

- **수평 확장**: 발송 서버 동적 증설 가능
- **피크 대응**: 평소 대비 **10배 트래픽** 처리
- **지역별 확장**: 글로벌 서비스 고려 (시간대)

**가용성 (Availability)**

- **서비스 가용률**: **99.9%** 이상
- **장애 복구**: 예약 데이터 유실 방지
- **발송 보장**: 장애 복구 후 놓친 예약 자동 처리
- **Graceful Degradation**: 부분 장애 시 핵심 기능 유지

**💭 고민 1: 미래 시점 작업 스케줄링**

수십만 건의 예약을 정확한 시간에 실행하려면?

**💭 고민 2: 피크 시간대 대응**

자정(00:00)에 수만 건이 동시 발송되어야 한다면?

**💭 고민 3: 취소와 환불 처리**

발송 직전에 취소 요청이 들어온다면?

**💭 고민 4: 발송 실패 처리**

발송 실패 시 재시도 전략은?

**💭 고민 5: 시간대(TimeZone) 처리**

한국에서 미국 친구에게 생일 자정 선물?

---

## 규모 파악하기

<aside>

| **일 예약 건수** | **5만건** | **50만건** |
| --- | --- | --- |
| 동시 접속자 | 5천명 | 5만명 |
| 시간당 발송 | 2천건 | 2만건 |

> **하루 5만~50만건의 예약, 동시 5천~5만명 접속**
> 

**1️⃣ 트래픽 관점**

- 하루 50만건 예약 등록 ⇒ **약 5.8TPS**
- DB단에서는 INSERT + SELECT 몇 건 ⇒ 약 **10QPS**
- 발송 피크(자정 30%) 때는 50만건 x 30% = **분 단위 15만건 ⇒ 약 2,500TPS**

⇒ 우선 순수 처리량만 봤을 땐 Spring Batch + DB + Redis 구성으로도 커버 가능하다고 생각함

 **2️⃣ 요구사항의 특성**

1. **트래픽이 일정하지 않다.**
    - 자정 / 명절 / 발렌타인데이처럼 특정 시점에 수만 건이 동시에 터짐
    ⇒ 이런 트래픽은 “평균치”가 아니라 “피크”가 시스템 안정성을 좌우
2. **“예약 발송”은 실시간 + 정시성이 중요**
    - “예약을 등록할 때”는 실시간 API 성능이 중요하고, “예약된 시각에 발송”은 정확한 타이밍(±1분)이 핵심
    ⇒  이를 단일 트랜잭션이나 DB 스케줄링으로만 처리하면, **장기 예약 1년치 데이터**를 모두 스캔해야할 수도 있음
3. **“발송”은 외부 API 연동**
    - 카카오톡 / SMS / 이메일 등 외부 통신은 100% 안정적이지 않음
    - 한 번에 수천 건을 외부 API에 던지면, 실패율이 급증하거나 장애 발생 가능
4. **서비스 확장과 장애 격리**
    - 예약, 발송, 알림, 환불 등 도메인 간 로직이 명확히 구분됨
    - 이를 하나의 모놀리식으로 묶으면, 예를 들어 발송 모듈 문제로 전체 예약 기능까지 다운될 수 있다고 생각함
</aside>

### **🔖 대략적인 전략 개요**

**Kafka 비동기 이벤트 처리 중심 + 부분 MSA** 

- **핵심 서비스(예약, 스케줄링, 발송)**를 독립 마이크로서비스로 분리
- 데이터 일관성은 **Outbox + Kafka 이벤트 스트림**으로 유지
- 나머지 알림/환불 등은 초기엔 동일 인스턴스 내 모듈로 통합 → 향후 트래픽 증가 시 별도 서비스로 분리 가능한 구조로 설계

### **🔖  요구사항의 특성 보완**

### **(1) 트래픽이 일정하지 않고, 피크 시간대 존재 ⇒ 평균 부하보다 피크 부하에 좀 더 중점을 둔 설계**

⇒ **Kafka 버퍼링 + 백프레셔 제어를 통한 트래픽 안정화** 

특정 시점에 집중되는 트래픽 패턴을 제어하기 위해, Kafka를 통해 이벤트를 큐잉하고 워커에서 점진적으로 처리한다. 
이를 통해 **예약 등록 서비스는 실시간 응답 속도를 유지**하면서도, **발송 처리 워커는 부하에 따라 자동 확장**되어 안정적인 처리를 지원한다. 

<aside>

### **Kafka 버퍼링**

> Kafka는 메시지를 ‘메모리’에 잠깐 두는 게 아니라, **디스크에 안전하게 쌓아두고 천천히 처리**할 수 있게 지원한다.
> 

**💡 왜 필요한가 ?** 
**자정**에 사용자 **10만명이 동시에** ‘선물 예약’을 등록한다고 해보자. 
**모든 이벤트**를 바로 **“발송 서비스”**로 보낸다면, **발송 워커가 감당할 수있는 처리량을 초과**한다면 장애가 발생한다.

```java
Reservation Service ⇒ 이벤트 발행 ⇒ [Kafka → 디스크에 순차 저장(버퍼링)] 
```

이런 식으로 **Kafka는 컨슈머가 읽을 때까지 안전하게 보관 가능**하며, **Delivery Serivce(컨슈머)는 처리 가능한 속도로** 가져가면 된다. 
결과적으로, **Kafka가 트래픽을 일시 저장해 과부하 구간을 흡수하는 버퍼 역할**을 ****한다. 

**[Kafka의 장점]**

- 대량 데이터를 **디스크 기반으로 지속 보관 (Retention 정책)**
- **순서 보장** (파티션 단위)
- **프로듀서와 컨슈머 속도 비동기화** 가능

### Kafka 백프레셔 제어

> 백프레셔(Backpressure)는 컨슈머의 처리 속도가 느릴 때, 이벤트 발행자(생산자)가 속도를 조절하는 메커니즘
> 

```java
Producer → Broker → Consumer
```

여기서 컨슈머의 처리 속도가 느려지면,

1. Kafka 브로커에 쌓이는 메시지 수가 늘어남
2. Consumer Lag(지연량)이 증가함
3. 모니터링 시스템(Prometheus 등)에서 **Lag 기준으로 자동 Scale-out** → **Consumer 인스턴스 수를 늘림**
4. Producer는 여전히 빠르게 이벤트를 쌓지만, Kafka의 **send buffer / ack 설정**으로 과도한 push를 제한할 수 있음

즉, Kafka는 내부적으로 **자연스러운 백프레셔 조절**을 수행한다. 

- **Consumer Lag 기반의 처리속도 모니터링**
- **auto-scaling 연동 (HPA 등)**
- **브로커 레벨 버퍼링 정책**
</aside>

### **(2) 서비스 확장과 장애 격리**

⇒ **핵심 서비스(선물예약, 선물 발송, 스케줄링) MSA로 분리**하여 **도메인별 독립 확장 / 장애 격리 및 운영 안정성 확보**

- 각 도메인이 가진 **부하 특성**(예약은 I/O 중심, 발송은 외부 네트워크 중심, 스케줄러는 CPU·메모리 중심)**이 다르기 때문에**, 이들을 독립 서비스로 분리해 도메인별 확장에 유리
- 특정 도메인의 트래픽이 급증해도 다른 서비스에 영향 전파되지 않기 위함
- 장애 발생 시 격리되어, 한 서비스의 오류가 전체 트랜잭션에 전파되지 않기 위함

### **(3) “예약 발송”은 실시간 + 정시성이 중요, 예약 리스트 풀 스캔**

⇒ **Kafka + Redis ZSET 캐싱**으로 **‘발송 임박 예약’ 만 큐에 저장**, **스케쥴러가 이벤트 기반으로 분 단위 정확도 보장**

🔖 **왜 이런 구조를 쓰는가?**

1. RDB 풀스캔 방지
예약이 100만건 누적되어 있을 경우, “지금 보낼 예약”을 찾기 위해 `WHERE scheduled_at <= NOW()` 쿼리를 계속 돌리면 **DB I/O 폭증 + 인덱스 캐시 오염
Redis ZSET은 메모리 기반 정렬**이기에 **O(logN)의 성능**으로 특정 예약을 찾을 수 있다. 

2. **Redis는 밀리초 단위 정렬이 가능**하기에, **정확히 예약된 시간 근처의 데이터만 Kafka로 발행이 가능**하다.

3. Redis 캐시가 날아가도, 장기 예약은 RDB에 남아 있다. Scheduler는 장애 복구 시 DB에서 “T~T+15분” 구간만 재적재하면 됨

**⇒ Scheduler 서비스는 ZSET**에서 발송 시각이 임박한 예약을 **Kafka로 발행**하고, **Delivery 서비스가 이를 소비하여 실제 발송을 수행**한다.
    이를 통해 **DB 부하를 최소화하면서 ±1분 내 정시 발송 정확도**를 확보한다.

🔖 **예상 흐름** 

1. SchedulerService가 **Kafka에서 새 예약 정보를 구독**

2. 예약 시간(`scheduled_at`)을 보고

- 현재 시각 기준 **15분 이후**면 → RDB에만 저장
- 15분 이내(임박)면 → Redis ZSET에 추가
    
    # ZSET 구조 예시
    
    ```java
    key: "schedule:2025-10-30"
    member: reservation_id
    score:  timestamp (epoch_ms)
    ```
    
    score 값이 ‘발송 예정 시각’에 해당한다. 
    

3. 스케쥴러가 Redis에서 꺼내 Kafka로 발행

- Scheduler가 1초~10초 주기로 동작하며 설정 범위의 예약들을 Redis에서 꺼냄
- 그 예약들을 **토픽에 발행**

4. Delivery 서비스에서 실제 발송

- DeliveryService가 Kafka 토픽 구독 ⇒ 메시지를 받으면 예약ID 기반으로 DB에서 정보 조회
- 카카오톡 / SMS / 이메일 API 호출

### **(4) “선물 발송”의 경우 외부 API 의존**

⇒ **Kafka Consumer**로 **재시도 / 백오프 / DLQ** 유연하게 설계 가능

```java
DB에서 발송 목록 조회 → 외부 API 호출 → 실패 → 예외 발생 → 전체 프로세스 중단
```

**외부 API를 통한 선물 발송 요청**을 동기 처리하게 되면, **한 번의 장애로 전체  트랜잭션**을 막아버릴 수 있다. 

```java
[Scheduler]
   └─ Kafka에 "발송 요청 이벤트" 발행 (예: gift.send.request)

[Delivery Consumer (Kafka Consumer)]
   └─ 메시지 수신 → 외부 API 호출 시도
       ├─ 성공 → OK (처리 끝)
       └─ 실패 → 재시도 큐로 보냄 (retry topic)
```

따라서, Kafka로 ‘발송해야 할 작업;을 하나하나 **이벤트로 쌓아두고**, **컨슈머가 순차적으로 처리하게 할 수 있다.**

<aside>

### Kafka Backoff 구조

> 컨슈머가 실패했을 때, 바로 다시 시도하면 또 같은 에러가 발생할 확률이 높기에, **Kafka**는 **지수적 백오프(Exponential Backoff)** 방식으로 점점 늦게 재시도한다.
> 

| 재시도 횟수 | 대기 시간 | 설명 |
| --- | --- | --- |
| 1회 | 10초 후 | 일시적 네트워크 오류 가능 |
| 2회 | 30초 후 | 서버 과부하 가능 |
| 3회 | 2분 후 | 연속 실패 시 일정 시간 후 재시도 |

각 토픽마다 Consumer 그룹이 있어서, 설정된 값만큼의 딜레이 후에 재시도를 수행하게 된다. 

### Kafka DLQ(Dead Letter Queue) 구조

> 특정 횟수 이상 반복해도 계속해서 실패한다면
이건 단순한 일시적 오류가 아닌, API or 데이터 문제일 가능성이 높다. 이때 **Kafka는 실패 메시지 전용 토픽인 DLQ로 해당 메시지를 옮겨버린다.**
> 

개발자는 여기서 실패 내역 조회가 가능하고, 문제 해결 후 수동으로 재처리할 수도 있다. 

</aside>

**🔖 선물 중복 발송에 대한 대비**

위와 같이 Kafka Backoff 구조를 통해 실패 후 재시도가 가능하다. 
이때, 아래와 같이 동일한 메시지가 **여러 번 처리되어 같은 선물이 중복 발송되는 상황에 대한 대비**가 필요하다. 

물론 멱등성 헤더와 서비스단의 내부 구현을 통해 멱등한 API로 만들 수 있지만, 
**Kafka 메시지 단위로** **멱등성 키(delivery_id)** 를 부여할 수 있다. 

```java
delivery_id = reservation_id + delivery_channel   // ex) 12345_KAKAO
```

## 기능별 설계 정리

### 1️⃣ 예약

**사용자 → ReservationService → DB + Outbox → Kafka**

**🔖 기능** 

- 사용자가 선물을 등록, 수정, 취소
- 발송 시간, 수신자, 메시지, 결제 정보 관리

**🔖 비기능적 요구사항**

정확성 : 예약 등록 시점에서 scheduled_at 검증 (최소 10분 후, 최대 1년 후) 결제 완료 후만 Kafka에 발행 → 데이터 정합성 확보

가용성 : **Outbox 패턴으로 예약 정보 유실 방지 &** **Kafka 브로커 장애 시 Outbox에서 재전송 보장**

취소/ 환불 : 예약 상태를 CREATED → SCHEDULED → DISPATCHED 단계로 관리, 스케줄러 진입 전까진 CANCELABLE로 유지하여 안전하게 취소 가능

```java
[Client] 
   ↓
[Reservation API] 
   └─ DB 저장 (예약 + 결제)
   └─ Outbox 이벤트 기록
   └─ Kafka("reservation.created") 발행
```

<aside>

### Transactional Outbox Pattern

![image.png](%5B5%EC%A3%BC%EC%B0%A8%5D%20%EC%B9%B4%EC%B9%B4%EC%98%A4%20%EC%84%A0%EB%AC%BC%ED%95%98%EA%B8%B0%20%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EC%84%A4%EA%B3%84/image.png)

**해당 패턴**은 상태 변경과 Outbox 테이블에 이벤트 레코드 삽입을 **하나의 트랜잭션**으로 묶어 **한 번의 쓰기로 원자성과 정합성을 보장**한다. 
분산 시스템에서 흔히 발생하는 **이중 쓰기 문제**를 해결하는 디자인 패턴이다.  (**이중 **쓰기 **문제 : 위의 두 작업이 **분리되어 수행될 때** 두 작업 간 동기화가 어긋나면서 정합성이 깨지는 상황)

이 패턴은 **서비스의 데이터 변경을 반영하는 이벤트**를 **메시지 브로커에 안전하게 발행**하면서, **트랜잭션의 일관성을 유지하고 장애 상황에서도 데이터 손실을 방지하는 데 중요한 역할**을 한다. 
(DB 쓰기와 이벤트 발행의 원자성 보장 + 최소 I/O)

</aside>

### 2️⃣ 스케줄러

- **예약 시간(Timestamp)**에 맞춰 **발송 요청 이벤트**를 Kafka로 발행

**🔖 비기능적 요구사항**

정확성 : Redis ZSET 정렬 기반으로 ±1분 내 발송 정확도 보장

확장성 : 예약 건수 증가 시 스케줄러 샤딩 (분 단위 파티션 키) ⇒ **파티셔닝**을 통한 병렬 처리

가용성 : Redis에 장애가 나더라도, Redis 접근이 불가능한 동안 Scheduler는 DB에서 직접 예약 데이터를 조회하여 이를 근거로 복구 ****(Redis는 근접 예약 캐싱 용도)

```java
[Kafka "reservation.created"]
   ↓
[Scheduler Service]
   ├─ 장기 예약 → DB 저장
   ├─ 근접 예약 → Redis ZSET(score=timestamp)
   ├─ 1초~10초 주기로 ZSET pop
   └─ Kafka("dispatch.request")토픽에 발행
```

```java
[Redis 정상]
  Reservation -> Scheduler -> Redis(ZSET) -> Kafka("dispatch")

[Redis 장애 발생]
  Scheduler → DB 직접 조회 (NOW ~ +15min) → Kafka("dispatch")

[Redis 복구 후]
  Scheduler → DB 재적재 (NOW ~ +30min) → Redis ZSET 복원
```

### 3️⃣ 선물 발송

- Kafka 소비자(Consumer)로서 발송 요청 이벤트를 처리
- 외부 API(KakaoTalk, SMS, Email) 호출 및 결과 관리
- 실패 시 백오프 기반 재시도 / DLQ로 이동

**🔖 비기능적 요구사항**

정확성 : `delivery_id = reservation_id + channel` 형태로 멱등성 보장

가용성 : Kafka 메시지 기반 → 장애 시 재처리 가능

확장성 : Kafka Consumer 그룹으로 수평 확장

발송 실패 시 : 재시도 → 백오프 → DLQ 순으로 처리

정확한 발송 시간 : Scheduler가 Kafka에 이벤트 발행 시각 기준으로 ±1분 내 소비 보장

```java
[Kafka "dispatch.request"]
   ↓
[Delivery Service]
   ├─ DB 조회 (예약정보)
   ├─ 외부 API 호출 (Kakao/SMS/Email)
   ├─ 성공 → delivery.success 이벤트
   ├─ 실패 → retry / DLQ
   └─ Kafka("delivery.result") 발행
```

### 4️⃣ 알림

- 발송자, 수신자에게 이벤트 알림 전송
- FCM / 카카오 / SMS / Email 등 다양한 채널 지원
- 만료 D-3, D-1 알림 스케줄링

**🔖 비기능적 요구사항**

정확성 : 예약/발송 상태 이벤트 기반으로 정확한 시점에 알림 발송

**확장성 : 알림 채널별로 Consumer 분리 (FCM / SMS / Kakao)**

가용성 : 알림 실패 시 재시도 및 DLQ 분리

```java
[Kafka "delivery.result" or "gift.expired.soon"]
   ↓
[Notification Service]
   ├─ 채널 라우팅 (FCM/SMS/Kakao)
   ├─ 재시도 / DLQ
   └─ 알림 로그 기록
```

### 5️⃣ 환불

- 미사용·만료된 선물의 자동 환불 처리
- 결제/정산 시스템과 연동
- 비정상 결제 롤백 및 보상 트랜잭션 수행

**🔖 비기능적 요구사항**

정확성 : 환불 금액 = 예약 결제 금액과 정확히 일치 

가용성 : 이벤트 기반으로 누락된 환불 요청 재처리 가능 

취소/환불 : 발송 전까지 취소 요청 시 결제 자동 환불, 발송 이후엔 만료 후 환불

장애 복구 : Outbox + 재시도로 결제 모듈 장애 시 보상 트랜잭션 수행

```java
[Kafka "gift.expired" or "reservation.cancelled"]
   ↓
[Refund Service]
   ├─ 결제 API 호출 (PG)
   ├─ 환불 내역 기록
   ├─ 성공 시 Kafka("refund.completed")
   └─ 실패 시 DLQ 이동
```

### **6️⃣ 시간대(TimeZone) 처리**

- 사용자별 지역시간대에 맞춰 발송 및 알림 시점 보정
****

**🔖 비기능적 요구사항**

정확성 : 예약시 timezone 필드를 함께 저장

확장성 : 글로벌 확장시 UTC 기준 + 나라별 오프셋 변환 로직 사용

피크 대응 : 국가별 자정 피크가 분산 → 파티션별로 트래픽 분산