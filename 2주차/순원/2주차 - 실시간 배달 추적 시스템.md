### **요구사항 분석**

**시스템 규모**

- **사용자**: MAU 1,000만명, DAU 300만명, 피크 동시접속 100만명
- **라이더**: 전체 100만명, 피크시간 활성 5만명
- **음식점**: 전체 50만개, 일일 활성 10만개
- **주문**: 일일 500만건, 피크시간 10만건/분

**성능 요구사항**

- **응답시간**: 주문생성 < 1초, 위치업데이트 < 500ms, 매칭 < 2초
- **처리량**: 주문 API 10,000 TPS, 위치 업데이트 20,000 TPS, 위치 조회 100,000 TPS
- **가용성**: 99.9% (핵심 기능은 99.95% 목표)

**위치 데이터 특성**

- **라이더 위치 업데이트**: 5초마다 (피크시간 5만명 → 10,000 TPS)
- **고객 위치 조회**: 10초마다 새로고침 (동시 50만명 → 50,000 TPS)
- **데이터 보관**: 상세 위치 24시간, 위치 이력 7일

---

## **인프라 규모 산정**

### **1. Redis (In-Memory 저장소)**

Redis는 실시간 라이더 위치 추적, 사용자 세션, 자주 조회되는 데이터 캐싱을 담당하여 시스템의 응답 속도를 극대화합니다.

### **가. 실시간 라이더 위치 데이터**

| 항목 | 데이터 타입 | 용량 (bytes) | 비고 |
| --- | --- | --- | --- |
| 라이더 ID | `Long` | 8 | Key로 사용 |
| 경도, 위도 | `Double` | 16 | GEOADD 명령어로 저장 |
| 타임스탬프 | `Long` | 8 |  |
| **Redis 자료구조 오버헤드** |  | **~60 bytes** | Sorted Set 내부 구조 등 |
| **라이더 1명당 총계** |  | **약 90 bytes** |  |
- **피크 시간 총 필요 용량**: 5만 명 (활성 라이더) × 90 bytes = **4.5 MB**

---

### **2. PostgreSQL (영구 저장소)**

PostgreSQL은 주문, 결제, 위치 이력 등 모든 트랜잭션 데이터를 안정적으로 저장하는 역할을 합니다.

### **가. 주문 1건당 상세 용량 분석 (평균 1.5 KB)**

주문 1건은 여러 테이블에 걸쳐 데이터가 생성됩니다.

- **`orders` 테이블 (1 Row, 약 290 bytes)**: 주문 ID, 사용자/가게/라이더 ID, 주소, 가격, 상태 등
- **`order_items` 테이블 (order 하나 당 평균 2.5 Row, 약 505 bytes)**: 주문된 각 메뉴의 이름, 수량, 가격, 옵션(JSONB) 등
- **`payments` 테이블 (1 Row, 약 136 bytes)**: 결제 ID, 수단, 상태, 거래 ID 등
1. **순수 데이터 합계**: 290 + 505 + 136 = **약 931 bytes**
2. **기타 요소 포함**:
    - **인덱스 공간**: 검색 성능 향상을 위한 인덱스가 약 30%의 추가 공간 사용
    - **MVCC 오버헤드**: `UPDATE`가 잦은 주문 상태로 인해 발생하는 Table Bloat
    - **안전 마진**: 데이터 구조 변경 및 예상치 못한 증가에 대비
3. **최종 산정**: 위 요소들을 모두 고려하여 주문 1건당 **평균 1.5 KB**

### **나. 위치 이력 테이블 (`location_history`, 7일 보관)**

| 항목 | 데이터 타입 | 용량 (bytes) | 비고 |
| --- | --- | --- | --- |
| `rider_id` | `BIGINT` | 8 |  |
| `longitude` | `DOUBLE PRECISION` | 8 |  |
| `latitude` | `DOUBLE PRECISION` | 8 |  |
| `created_at` | `TIMESTAMPTZ` | 8 |  |
| **순수 데이터** |  | **32 bytes** |  |
| **Row Header** | - | **24 bytes** | PostgreSQL 내부 헤더 |
| **안전 마진/패딩** | - | **8 bytes** | 확장성 및 정렬을 고려 |
| **레코드 1개당 총계** |  | **약 64 bytes** |  |

### **다. 최종 PostgreSQL 스토리지 용량**

- **주문 데이터 (1년)**: 500만 건/일 × 1.5 KB × 365일 = **약 2.74 TB**
- **위치 이력 데이터 (7일)**: 2.88억 건/일 × 64 bytes × 7일 = **약 129 GB**
- **인덱스 총량 (30%)**: (2.74 TB + 129 GB) × 0.3 ≈ **860 GB**
- **기타 테이블 (users, stores 등)**: **약 200 GB**
- **총 필요 용량**: 2.74 TB + 129 GB + 860 GB + 200 GB ≈ **3.9 TB**
- **최종 권장 사양**: Read Replica, 백업 등을 고려하여 **7~8 TB 스토리지** 구성

### **라. IOPS 요구사항**

- **Write IOPS**: 주문 생성 및 위치 이력 저장 → **약 12,000 IOPS**
- **Read IOPS**: 주문 조회, 사용자 정보 등 읽기 작업 → **약 20,000 IOPS**
- **총 필요 IOPS**: 최소 32,000 IOPS 이상

---

### **핵심 기술적 도전과제**

**1. Hot Spot 문제 → Geosharding 전략**

- **문제**: 강남역과 같이 주문, 라이더가 특정 지역에 집중되어 해당 지역을 담당하는 서버에 부하가 몰리는 현상(Hot Spot)이 발생
- **해결방안**: S2 Geometry나 Geohash 같은 지리 공간 분할 라이브러리를 사용합니다. 지도를 일정한 크기의 셀로 분할하고, 각 셀 또는 셀 그룹을 별도의 서버에 할당합니다. 라이더 위치가 업데이트되면 Geohash를 계산하여 해당 Shard로 직접 라우팅하고, 주문 매칭 시에도 음식점 위치가 속한 셀과 인접 셀만 조회하여 검색 범위를 최적화합니다. 이를 통해 Hot Spot 지역만 선택적으로 Scale-out이 가능해집니다.

**2. 상태 일관성 → 이벤트 기반 아키텍처**

- **문제**: MSA 환경에서는 주문 상태가 변경될 때 고객, 라이더, 음식점, 관리 시스템 등 여러 서비스의 데이터 동기화를 보장하기 어렵습니다.
- **해결방안**: **주문 서비스**를 모든 상태 변경의 단일 진실 공급원(Source of Truth)으로 지정하고, 상태가 변경될 때마다 이벤트를 발행하는 방식을 사용합니다.
    - **흐름 예시 (픽업 완료)**:
        1. 라이더가 '픽업 완료' 요청을 보냅니다.
        2. 주문 서비스는 DB 트랜잭션 내에서 주문 상태를 '배달중'으로 변경합니다.
        3. 트랜잭션이 성공하면, `OrderPickedUp` 이벤트를 Kafka에 발행합니다.
        4. 알림 서비스, 위치 서비스 등 다른 서비스들은 이 이벤트를 구독(Subscribe)하여 '라이더 픽업 알림 발송', '실시간 위치 추적 허용' 등 후속 조치를 비동기적으로 처리합니다.

---

### **MSA 설계 이유**

**1. 서비스별 독립적 확장**

- 모놀리식 아키텍처에서는 위치 업데이트(20,000 TPS)와 같이 특정 기능의 부하가 높아져도 전체 애플리케이션을 통째로 확장해야 하므로 자원 낭비가 심합니다.
- MSA에서는 부하가 높은 위치 서비스만 수십 대로 확장하고, 상대적으로 부하가 적은 다른 서비스는 최소한의 자원으로 운영하여 비용 효율성을 극대화할 수 있습니다.

**2. 장애 격리**

- 모놀리식 아키텍처에서는 리뷰 기능의 버그가 전체 시스템을 중단시켜 핵심 기능인 주문과 배달까지 마비시킬 수 있습니다.
- MSA에서는 리뷰 서비스에 장애가 발생하더라도 주문, 결제, 위치 추적 등 다른 핵심 서비스는 정상적으로 동작하여 전체 시스템의 가용성을 높게 유지할 수 있습니다.

**3. 기술 선택의 유연성**

- MSA를 통해 각 서비스의 특성에 맞는 최적의 기술 스택을 선택할 수 있습니다. 예를 들어, 머신러닝 기반의 매칭 서비스는 Python으로, 대용량 트랜잭션 처리가 중요한 주문 서비스는 Java/Kotlin으로, 실시간 통신이 중요한 알림 서비스는 NodeJS로 개발할 수 있습니다.

---

### **지오펜싱 기반 도착 알림**

1. **가상 울타리 생성**: 주문 상태가 '배달중'으로 변경되면, 고객의 배달 주소를 중심으로 반경 100m의 가상 울타리(Geofence)를 생성하여 Redis에 저장합니다.
2. **서버에서 거리 계산**: 5초마다 수신되는 라이더의 위치 업데이트를 처리할 때, 해당 라이더가 배달 중인 주문의 가상 울타리 정보가 있는지 확인합니다. 울타리가 존재하면, 현재 위치와 울타리 중심(고객 집) 간의 거리를 이용해 계산합니다.
3. **즉시 알림 발송**: 계산된 거리가 100m 안으로 들어오는 순간, '진입'으로 감지합니다. 이때 "곧 도착합니다!" 푸시 알림을 단 한 번만 발송하고, 알림 발송 완료 플래그를 기록하여 중복 발송을 방지합니다.
